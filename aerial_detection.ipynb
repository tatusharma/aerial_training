{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.203-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-macosx_13_0_arm64.whl.metadata (19 kB)\n",
      "Collecting pillow\n",
      "  Downloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting numpy>=1.23.0 (from ultralytics)\n",
      "  Downloading numpy-2.3.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting pyyaml>=5.3.1 (from ultralytics)\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting requests>=2.23.0 (from ultralytics)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting scipy>=1.4.1 (from ultralytics)\n",
      "  Downloading scipy-1.16.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting torch>=1.8.0 (from ultralytics)\n",
      "  Downloading torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting torchvision>=0.9.0 (from ultralytics)\n",
      "  Downloading torchvision-0.23.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.13/site-packages (from ultralytics) (7.1.0)\n",
      "Collecting polars (from ultralytics)\n",
      "  Downloading polars-1.33.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.23.0 (from ultralytics)\n",
      "  Downloading numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.23.0->ultralytics)\n",
      "  Downloading charset_normalizer-3.4.3-cp313-cp313-macosx_10_13_universal2.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.23.0->ultralytics)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.23.0->ultralytics)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.23.0->ultralytics)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting filelock (from torch>=1.8.0->ultralytics)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting setuptools (from torch>=1.8.0->ultralytics)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.8.0->ultralytics)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch>=1.8.0->ultralytics)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.8.0->ultralytics)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.8.0->ultralytics)\n",
      "  Downloading markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Downloading ultralytics-8.3.203-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.12.0.88-cp37-abi3-macosx_13_0_arm64.whl (37.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m  \u001b[33m0:00:15\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.6-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading pandas-2.3.2-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl (274 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.0-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl (64 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp313-cp313-macosx_10_13_universal2.whl (205 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading scipy-1.16.2-cp313-cp313-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading torchvision-0.23.0-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading polars-1.33.1-cp39-abi3-macosx_11_0_arm64.whl (35.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: pytz, mpmath, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, setuptools, pyyaml, pyparsing, polars, pillow, numpy, networkx, MarkupSafe, kiwisolver, joblib, idna, fsspec, fonttools, filelock, cycler, charset_normalizer, certifi, scipy, requests, pandas, opencv-python, jinja2, contourpy, torch, scikit-learn, matplotlib, ultralytics-thop, torchvision, ultralytics\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37/37\u001b[0m [ultralytics]\u001b[0m [ultralytics]]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 certifi-2025.8.3 charset_normalizer-3.4.3 contourpy-1.3.3 cycler-0.12.1 filelock-3.19.1 fonttools-4.60.0 fsspec-2025.9.0 idna-3.10 jinja2-3.1.6 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 mpmath-1.3.0 networkx-3.5 numpy-2.2.6 opencv-python-4.12.0.88 pandas-2.3.2 pillow-11.3.0 polars-1.33.1 pyparsing-3.2.5 pytz-2025.2 pyyaml-6.0.3 requests-2.32.5 scikit-learn-1.7.2 scipy-1.16.2 setuptools-80.9.0 sympy-1.14.0 threadpoolctl-3.6.0 torch-2.8.0 torchvision-0.23.0 tqdm-4.67.1 typing-extensions-4.15.0 tzdata-2025.2 ultralytics-8.3.203 ultralytics-thop-2.0.17 urllib3-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics opencv-python pillow matplotlib tqdm pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clened images saved. Good: 32823, Bad: 0\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"dataset/images\")\n",
    "image_out = Path(\"dataset/clean_img\")\n",
    "def is_image_ok(path):\n",
    "    try:\n",
    "        with Image.open(path) as im:\n",
    "            im.verify()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    \n",
    "hashes = {}\n",
    "good, bad = 0,0\n",
    "for img in data_dir.rglob(\"*.jpg\"):\n",
    "    if not is_image_ok(img):\n",
    "        print(\"Corrupted\",img)\n",
    "        bad += 1\n",
    "        continue\n",
    "    h = hash(img.read_bytes())\n",
    "    if h in hashes:\n",
    "        print(\"Duplicate\",img, \"==\",hashes[h])\n",
    "        bad += 1\n",
    "        continue\n",
    "    hashes[h] = str(img)\n",
    "    shutil.copy(img,image_out / img.name)\n",
    "    good += 1\n",
    "print(f\"Clened images saved. Good: {good}, Bad: {bad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "hashes = {}\n",
    "for img_path in data_dir.glob(\"*.jpg\"):\n",
    "    with open(img_path, \"rb\") as f:\n",
    "        filehash = hashlib.md5(f.read()).hexdigest()\n",
    "    if filehash in hashes:\n",
    "        print(f\"Duplicate: {img_path} and {hashes[filehash]}\")\n",
    "    else:\n",
    "        hashes[filehash] = img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level keys: dict_keys(['info', 'licenses', 'categories', 'annotations'])\n",
      "\n",
      "Number of annotations: 32823\n",
      "Number of categories: 8\n",
      "\n",
      "Sample annotation:\n",
      "{'altitude': 19921.6,\n",
      " 'angle_phi': -0.06713105738162994,\n",
      " 'angle_psi': 1.1161083340644837,\n",
      " 'angle_theta': 0.06894744634628296,\n",
      " 'bbox': [{'class': 1, 'height': 185, 'left': 1098, 'top': 163, 'width': 420},\n",
      "          {'class': 1, 'height': 176, 'left': 1128, 'top': 421, 'width': 393},\n",
      "          {'class': 0, 'height': 153, 'left': 1703, 'top': 927, 'width': 183}],\n",
      " 'image_height': 1080.0,\n",
      " 'image_name': 'frame_20190829091111_x_0001973.jpg',\n",
      " 'image_width:': 1920.0,\n",
      " 'latitude': 56.20630134795274,\n",
      " 'linear_x': 0.03130074199289083,\n",
      " 'linear_y': 0.028357808757573367,\n",
      " 'linear_z': 0.0744575835764408,\n",
      " 'longtitude': 10.18798203255313,\n",
      " 'platform': 'Parrot Bebop 2',\n",
      " 'time': {'day': 29,\n",
      "          'hour': 9,\n",
      "          'min': 11,\n",
      "          'month': 8,\n",
      "          'ms': 394400.0,\n",
      "          'sec': 11,\n",
      "          'year': 2019}}\n",
      "\n",
      "Sample category:\n",
      "'Human'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "ann_json = \"dataset/annotations.json\"\n",
    "\n",
    "with open(ann_json, 'r') as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "print(\"Top-level keys:\", ann.keys())\n",
    "\n",
    "print(\"\\nNumber of annotations:\", len(ann['annotations']))\n",
    "print(\"Number of categories:\", len(ann['categories']))\n",
    "\n",
    "# Show a sample annotation\n",
    "print(\"\\nSample annotation:\")\n",
    "pprint(ann['annotations'][0])\n",
    "\n",
    "# Show a sample category\n",
    "print(\"\\nSample category:\")\n",
    "pprint(ann['categories'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 4923 images to dataset/images_subset\n",
      "Subset annotations saved.\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "data_dir = Path(\"dataset/images\")\n",
    "image_out = Path(\"dataset/images_subset\")\n",
    "image_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load annotations\n",
    "with open(\"dataset/annotations.json\", \"r\") as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "annotations = ann['annotations']\n",
    "\n",
    "# Take 15% of dataset\n",
    "subset_size = int(0.15 * len(annotations))\n",
    "subset_annotations = random.sample(annotations, subset_size)\n",
    "\n",
    "# Collect unique image names\n",
    "subset_image_names = set(a['image_name'] for a in subset_annotations)\n",
    "\n",
    "# Copy images to new folder\n",
    "for img_name in subset_image_names:\n",
    "    src = data_dir / img_name\n",
    "    dst = image_out / img_name\n",
    "    if src.exists():\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "print(f\"Copied {len(subset_image_names)} images to {image_out}\")\n",
    "\n",
    "# Save new subset JSON\n",
    "subset_json = {\n",
    "    \"info\": ann[\"info\"],\n",
    "    \"licenses\": ann[\"licenses\"],\n",
    "    \"categories\": ann[\"categories\"],\n",
    "    \"annotations\": subset_annotations\n",
    "}\n",
    "\n",
    "with open(image_out / \"annotations_subset.json\", \"w\") as f:\n",
    "    json.dump(subset_json, f, indent=2)\n",
    "\n",
    "print(\"Subset annotations saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO labels created in data/yolo/labels_all\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "LABELS_DIR = Path(\"data/yolo/labels_all\")\n",
    "LABELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLASSES = [\"car\",\"truck\",\"person\"]  # adjust based on dataset\n",
    "class_map = {c:i for i,c in enumerate(CLASSES)}\n",
    "\n",
    "def voc_to_yolo(xml_file, img_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    w = int(root.find(\"size/width\").text)\n",
    "    h = int(root.find(\"size/height\").text)\n",
    "    lines = []\n",
    "    for obj in root.findall(\"object\"):\n",
    "        cls = obj.find(\"name\").text\n",
    "        if cls not in class_map:# =============================================\n",
    "# 5. Create train/val/test splits (only 15% of dataset)\n",
    "# =============================================\n",
    "SPLIT_DIR = Path(\"data/yolo\")\n",
    "for split in [\"train\",\"val\",\"test\"]:\n",
    "    (SPLIT_DIR / \"images\" / split).mkdir(parents=True, exist_ok=True)\n",
    "    (SPLIT_DIR / \"labels\" / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Take only 15% of cleaned images\n",
    "images = list(IMAGES_OUT.glob(\"*.jpg\"))\n",
    "random.shuffle(images)\n",
    "subset_size = int(0.15 * len(images))\n",
    "images = images[:subset_size]\n",
    "print(f\"Using {len(images)} images out of {len(list(IMAGES_OUT.glob('*.jpg')))} (~15%)\")\n",
    "\n",
    "# Split into train (80%), val (10%), test (10%)\n",
    "n = len(images)\n",
    "train, val, test = np.split(images, [int(.8*n), int(.9*n)])\n",
    "\n",
    "splits = {\"train\":train,\"val\":val,\"test\":test}\n",
    "\n",
    "for split, imgs in splits.items():\n",
    "    for img in imgs:\n",
    "        shutil.copy(img, SPLIT_DIR/\"images\"/split/img.name)\n",
    "        lbl = LABELS_DIR / (img.stem + \".txt\")\n",
    "        if lbl.exists():\n",
    "            shutil.copy(lbl, SPLIT_DIR/\"labels\"/split/lbl.name)\n",
    "        else:\n",
    "            (SPLIT_DIR/\"labels\"/split/lbl.name).write_text(\"\")\n",
    "\n",
    "print(\"Filtered and split dataset saved in data/yolo/\")\n",
    "\n",
    "            continue\n",
    "        cls_id = class_map[cls]\n",
    "        b = obj.find(\"bndbox\")\n",
    "        xmin, ymin, xmax, ymax = [float(b.find(x).text) for x in [\"xmin\",\"ymin\",\"xmax\",\"ymax\"]]\n",
    "        x_center = ((xmin+xmax)/2)/w\n",
    "        y_center = ((ymin+ymax)/2)/h\n",
    "        bw = (xmax-xmin)/w\n",
    "        bh = (ymax-ymin)/h\n",
    "        lines.append(f\"{cls_id} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}\")\n",
    "    return lines\n",
    "\n",
    "ANN_DIR = data_dir / \"annotations/voc\"  # adjust path\n",
    "for xml in ANN_DIR.glob(\"*.xml\"):\n",
    "    img_path = image_out / (xml.stem + \".jpg\")\n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    lines = voc_to_yolo(xml, img_path)\n",
    "    (LABELS_DIR / f\"{xml.stem}.txt\").write_text(\"\\n\".join(lines))\n",
    "\n",
    "with open(\"data/yolo/classes.txt\",\"w\") as f:\n",
    "    f.write(\"\\n\".join(CLASSES))\n",
    "\n",
    "print(\"YOLO labels created in\", LABELS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
